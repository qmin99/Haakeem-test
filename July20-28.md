# Documentation of Click-2-Talk Stuff (Last Update: July 28, 25)


## Record Approach ❌

Frontend Records Audio as a file → Send to Backend/Server/API(main.py) → HTTP Uploads to /voice/upload → Convert audio file extension → API Processes Everything (STT→LLM→TTS) → Sends Response to LiveKit Room → Frontend Receives Response and speaks using audio player package  ❌

---

Backend (FastAPI app - the server main.py) -> Handles audio processing, transcription, AI responses, and TTS synthesis FOR PTT ❌

It should be handled in the worker (agent.py) where the agent processing stuff are (as the NORMAL MODE) only after the user clicks SEND (or maybe SPEAK later!)
❌

---

What I remember that has been tried for that:

- Frontend records audio → Uploads to agent via LiveKit data channel ❌
- Session Registry Communication between Worker & Server ❌
- Frontend records audio → WebSocket to backend ❌
- base64 thing ❌
- Events thing Architecture ❌ 
- Other Minor Approaches ❌
- Normal Live Streaming with turn_detector= manual ❌
- many others ❌
- For July 29,30 I will try changing Livekit arch as to get stuff only as text and get them by Flutter package and use deepgram as fallback (just for PTT/Record thing mode) ❌



---

## Current Flow

### **1. Audio Recording (Frontend)**
```dart
// PTT Recording States
enum RecordingState { idle, recording, recorded, uploading }

// Audio capture using:
- Web: JavaScript MediaRecorder API
- Mobile: record package
- Live transcription during recording
```

### **2. Audio Upload (Frontend → Backend)**
```dart
// Uploads to: /voice/upload endpoint
// Supports: webm, m4a, wav, mp3 formats
// Returns: transcript, reply, speech_audio (base64)
```

### **3. Speech-to-Text (Backend)**
```python
# Deepgram API integration
async def transcribe_with_deepgram(audio_data: bytes) -> str:
    # Converts audio to 16kHz PCM WAV
    # Uses Nova-2 model for high accuracy
    # Returns transcript text
```

### **4. AI Response Generation (Backend)**
```python
# Groq LLaMA integration
async def generate_groq_response(prompt: str) -> str:
    # Uses llama3-8b-8192 model
    # Generates concise, helpful responses
    # Returns reply text
```

### **5. Text-to-Speech (Backend)**
```python
# Azure Speech Service integration
async def generate_azure_speech(text: str) -> bytes:
    # Uses en-US-JennyNeural voice
    # Outputs 16kHz 32kbps MP3
    # Returns audio bytes
```

### **6. Audio Playback (Frontend)**
```dart
// AudioPlayer integration
Future<void> _playSpeechAudio(String base64Audio):
    # Decodes base64 audio
    # Plays response using audioplayers package
    # Handles cross-platform audio playback
```

---

### **Recent Backend Dependencies**
```txt
# Core AI Services
azure-cognitiveservices-speech>=1.34.0  # TTS
livekit-agents[deepgram,groq,azure]     # STT, LLM, TTS
aiohttp>=3.8.0                          # HTTP client

# Audio Processing
pydub>=0.25.1                           # Audio manipulation
ffmpeg-python                           # Audio conversion

# Web Framework
fastapi==0.111.0                        # API framework
uvicorn[standard]==0.30.0               # ASGI server
```

### **Frontend Dependencies**
```yaml
# Audio & Speech
audioplayers: ^6.0.0                    # Audio playback
record: ^5.1.2                          # Audio recording
speech_to_text: ^7.0.0                  # Live transcription

# LiveKit Integration
livekit_client: ^2.4.9                  # Real-time communication
livekit_components: ^1.2.2+hotfix.1     # UI components

# Utilities
permission_handler: ^11.3.0             # Microphone permissions
http: ^1.3.0                            # HTTP requests
```

---

## Mode 

### **PTT Recording Widget**
- **Idle State**: "Tap to record" with microphone icon
- **Recording State**: Animated pulse, live transcription, cancel/stop buttons
- **Ready State**: Duration display, send/retry options
- **Uploading State**: Progress indicator with cloud upload icon

### **Mode Switching**
- **Call Mode**: Continuous listening with real-time transcription
- **PTT Mode**: Manual recording with longer audio support (up to 10 minutes)

### **Live Transcription**
- Real-time speech-to-text during PTT recording
- Visual feedback with transcription bubbles
- Automatic restart if recognition stops

---


---

## Added/Fixed July 28 ONLY


- PTT LIVE TRANSCRIPTION
- MODE SWITCHING ISSUE
- GLITCHES OF NEW UI
- DEBUGGED AND TESTED -> worked perfect as supposed but this arch is NOT OPTIMAL! as it gets responses straight from the api /voice/upload endpoint not from the worker using engineered algorithms there for SST LLM TTS File conversion etc which all should be streamed from the worker not the server -> which make it not optimal and just 1-message memory for that mode. 

---

## NEW MULTI-AGENT SYSTEM (July 29-30, 2025)

### **Architecture Overview**
**SOLVED THE OPTIMAL ARCHITECTURE ISSUE!** Now using LiveKit Agents Framework with proper worker-based processing:

```
Frontend (Flutter) ↔ LiveKit Room ↔ Multi-Agent Worker (agent.py)
                                    ├── AttorneyAgent (Continuous VAD)
                                    └── ClickToTalkAgent (Manual Control)
```

### **Two Distinct Agents in Single Worker**

#### **1. AttorneyAgent (Continuous Conversation)**
```python
class AttorneyAgent(Agent):
    # Turn Detection: VAD with semantic endpointing
    # Voice: en-US-DavisNeural (Professional male)
    # Behavior: Continuous conversation, interruptible
    # Use Case: Legal Q&A, document review, case analysis
```

**Features:**
- **VAD Turn Detection**: `min_endpointing_delay=3.3s`, `max_endpointing_delay=5.0s`
- **Interruption Support**: `allow_interruptions=True`, `discard_audio_if_uninterruptible=True`
- **Semantic Understanding**: Waits for complete thoughts before responding
- **Professional Voice**: DavisNeural for authoritative legal assistant tone

#### **2. ClickToTalkAgent (Manual Control)**
```python
class ClickToTalkAgent(Agent):
    # Turn Detection: Manual (user-controlled)
    # Voice: en-US-OnyxTurboMultilingualNeural (Fast, clear)
    # Behavior: Wait for user to finish, then process
    # Use Case: Long-form legal questions, detailed case descriptions
```

**Features:**
- **Manual Turn Detection**: User clicks "Start Speaking" → "Send to HAAKEEM"
- **No Interruption**: Agent waits for complete user input
- **Long Audio Support**: Can handle extended speech (up to 10+ minutes)
- **Fast Voice**: OnyxTurbo for quick, clear responses

### **Real-Time Communication**

#### **Frontend → Backend (Data Messages)**
```dart
// Agent Switching
await lp.publishData(utf8.encode('switch_to_attorney'));
await lp.publishData(utf8.encode('switch_to_click_to_talk'));

// Click-to-Talk Controls
await lp.publishData(utf8.encode('start_turn'));    // Start recording
await lp.publishData(utf8.encode('end_turn'));      // Process & respond
await lp.publishData(utf8.encode('cancel_turn'));   // Discard recording
await lp.publishData(utf8.encode('interrupt_agent')); // Force stop agent
```

#### **Backend Processing**
```python
# Data message handler in entrypoint
async def handle_data_packet(data: rtc.DataReceived):
    message = data.data.decode('utf-8')
    
    if message == "switch_to_attorney":
        await start_agent_session("attorney")
    elif message == "start_turn":
        session.interrupt()  # Stop current agent
        session.input.set_audio_enabled(True)  # Start listening
    elif message == "end_turn":
        session.input.set_audio_enabled(False)  # Stop listening
        # Agent processes and responds automatically
```

### **Technical Implementation**

#### **Agent Session Management**
```python
def start_agent_session(agent_type: str):
    # Clean up previous session
    if session:
        session.interrupt()  # Force stop
        await session.close()
    
    # Create new session with appropriate config
    if agent_type == "attorney":
        session = AgentSession(
            vad=VAD_MODEL,
            min_endpointing_delay=3.3,
            max_endpointing_delay=5.0,
            allow_interruptions=True,
            discard_audio_if_uninterruptible=True,
        )
        current_agent = AttorneyAgent()
    else:  # click_to_talk
        session = AgentSession(
            turn_detection="manual",
            discard_audio_if_uninterruptible=True,
        )
        current_agent = ClickToTalkAgent()
```

#### **Permanent Interruption System**
```python
# Multiple interrupt calls for complete stop
session.interrupt()  # Stop current speech
session.interrupt()  # Double interrupt for safety
session.clear_user_turn()  # Clear any pending input

# Clear output buffers
if hasattr(session, 'output') and hasattr(session.output, 'clear'):
    session.output.clear()
```

### **Frontend UI Components**

#### **Agent Selection Widget**
```dart
class AgentSelectionWidget extends StatelessWidget {
  // Two buttons: "Attorney AI" and "Click-to-Talk"
  // Shows current selection and agent description
  // Handles agent switching via AppCtrl
}
```

#### **Click-to-Talk Controls**
```dart
class ClickToTalkControls extends StatelessWidget {
  // States: idle, listening, readyToSend, processing
  // Buttons: "Start Speaking", "End Speaking", "Discard", "Send to HAAKEEM"
  // Visual feedback for current state
}
```

### **Error Handling & Logging**

#### **Transcription Error Suppression**
```python
class TranscriptionWarningFilter(logging.Filter):
    """Suppress transcription warnings when room is closed"""
    def filter(self, record):
        if 'failed to publish transcription' in record.getMessage():
            return False
        if 'room closed' in record.getMessage().lower():
            return False
        return True

# Apply to livekit.agents logger
livekit_logger.addFilter(TranscriptionWarningFilter())
```

#### **Connection State Handling**
```python
# Correct enum usage
if ctx.room.connection_state == rtc.ConnectionState.CONN_CONNECTED:
    # Safe to publish transcriptions
    pass
```

### **Key Advantages Over Previous Architecture**

1. **✅ Optimal Processing**: All STT→LLM→TTS happens in worker, not server
2. **✅ Multi-Message Memory**: Agents maintain conversation context
3. **✅ Real-Time Streaming**: No HTTP upload/download delays
4. **✅ Proper Interruption**: Permanent stops, no resume after interruption
5. **✅ Semantic Understanding**: Attorney agent waits for complete thoughts
6. **✅ Manual Control**: Click-to-talk for long-form questions
7. **✅ Professional Voices**: Different voices for different use cases
8. **✅ Clean Error Handling**: Suppressed transcription warnings

### **Deployment Configuration**
```python
# Worker options for production
WorkerOptions(
    agent_name="agent-HAAKEEM",  # Must start with "agent-"
    job_executor_type=JobExecutorType.THREAD,
    concurrency=1,
    concurrency_mode='threads',
    num_idle_processes=0,
    prewarm_fnc=prewarm,
)
```

### **Environment Variables**
```bash
# Backend .env
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret
LIVEKIT_URL=wss://your-livekit-server.com

# AI Services
DEEPGRAM_API_KEY=your_deepgram_key
GROQ_API_KEY=your_groq_key
AZURE_SPEECH_KEY=your_azure_key
AZURE_SPEECH_REGION=your_azure_region
```

---

**Status**: ✅ **PRODUCTION READY** - Multi-agent system fully functional with optimal architecture! 